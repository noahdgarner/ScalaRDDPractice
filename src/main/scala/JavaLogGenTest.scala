import org.apache.spark
import org.apache.spark.SparkContext

/**
  * This file was just for testing a reduce function on the random sentences
  * generated by the Java method
  */

object JavaLogGenTest {
  def main(args: Array[String]): Unit = {

    val sc = new SparkContext("local[8]","JavaLogGenTest")

    val logData = sc
      .textFile("src/main/resources/logFiles/logs.txt")

    //map randSentence to 1
    val mappedData = logData
      .map(x => (x.split(" ").drop(3).mkString(" "),1))

    //reduce on Sentence to see most occurring 'random sentences'
    val reducedDataCounted = mappedData
      .reduceByKey((x,y) => x+y);

    //sort by most occuring
    val sortedData = reducedDataCounted
      .sortBy(_._2, false)

    sortedData
      .take(10)
      .foreach(println)

  }
}
